{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T021 ·  One-Hot Encoding\n",
    "\n",
    "Developed in the CADD seminar 2020, Volkamer Lab, Charité/FU Berlin \n",
    "\n",
    "Authors :\n",
    "\n",
    "- Sakshi Misra, CADD seminar 2020, Charité/FU Berlin\n",
    "- Talia B. Kimber, 2020, [Volkamer lab](https://volkamerlab.org), Charité\n",
    "- Yonghui Chen, 2020, [Volkamer lab](https://volkamerlab.org), Charité\n",
    "- Andrea Volkamer, 2020, [Volkamer lab](https://volkamerlab.org), Charité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "\n",
    "The aim of the talktorial is to perform one-hot encoding of SMILES structures on a subset of the ChEMBL dataset to gain a deeper understanding on the one-hot encoding concept and why it is useful as a pre-processing step in various machine learning algorithms. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/logo.PNG\" style=\"max-width: 300px; width:50%;\" />\n",
    "</div>\n",
    "\n",
    "*Figure 1* : One-hot encoding in python. The figure is taken from the [blogpost](https://towardsdatascience.com/learning-one-hot-encoding-in-python-the-easy-way-665010457ad9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "- Molecular data and representation\n",
    "    - ChEMBL database\n",
    "    - SMILES structures and rules\n",
    "- What is categorical data?\n",
    "     - What is the problem with categorical data?\n",
    "     - How to convert categorical data to numerical data?\n",
    "- The One-Hot Encoding concept\n",
    "     - Why using one-hot encoding?\n",
    "     - Example of one-hot encoding\n",
    "     - Advantages of one-hot encoding\n",
    "     - Disadvantages of one-hot encoding\n",
    "- Other similar transformation: Integer encoding (label encoder)\n",
    "- Difference between label and one-hot encoding\n",
    "- Padding\n",
    "- Further readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "- Import necessary packages\n",
    "- Load data, draw molecules and visualize the dataframe\n",
    "- Apply one-hot encoding using own implementation\n",
    "  - Visualization of one-hot encoded matrix (unequal dimension)\n",
    "  - Discussion of shortcomings of defined *smiles_encoder* function\n",
    "- Pre-process SMILES data to overcome shortcomings\n",
    "- Apply one-hot encoding using own implementation on preprocessed data\n",
    "  - Visualization of one-hot encoded matrix (equal dimension)\n",
    "- Apply one-hot encoding using implementation in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "   - Without padding (unequal dimension) and visualization of the matrix\n",
    "   - With padding (equal dimension)\n",
    "       - Padding after one-hot encoding is performed and visualization of the matrix\n",
    "       - Padding before one-hot encoding performed and visualization of the matrix\n",
    "- Apply one-hot encoding using implementation in [keras](https://keras.io/)\n",
    "  - Without padding (unequal dimension) and visualization of the matrix\n",
    "  - With padding (equal dimension) and visualization of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Theoretical background:\n",
    "     - ChEMBL database: \"The ChEMBL bioactivity database: an update.\" ([<i>Nucleic acids research<i> (2014), <b>42.D1</b>, D1083-D1090](https://doi.org/10.1093/nar/gkt1031))\n",
    "     - Blogpost: Jason Brownlee, *How to One Hot Encode Sequence Data in Python*, [Machine Learning Mastery, accessed November 9th, 2020](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/).\n",
    "     - Blogpost: Krishna Kumar Mahto, *One-Hot-Encoding, Multicollinearity and the Dummy Variable Trap*, towardsdatascience, Available from [one-hot-encoding-multicollinearity](https://towardsdatascience.com/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a/), accessed July 8th, 2019.\n",
    "     - Blogpost: Chris, *What is padding in a neural network?*, MACHINECURVE, Available from [Padding](https://www.machinecurve.com/index.php/2020/02/07/what-is-padding-in-a-neural-network/#:~:text=Padding%20avoids%20the%20loss%20of%20spatial%20dimensions,-Sometimes%2C%20however%2C%20you&text=You%20need%20the%20output%20images,in%20order%20to%20generate%20them.), accessed February 7th, 2020\n",
    "     \n",
    "\n",
    "- Packages and functions:\n",
    "     - [**rdkit**](https://www.rdkit.org/docs/GettingStartedInPython.html): Greg Landrum,  *RDKit Documentation*, [PDF](https://buildmedia.readthedocs.org/media/pdf/rdkit/latest/rdkit.pdf), Release on 2019.09.1.\n",
    "     - [**scikit-learn**](https://scikit-learn.org/stable/): \n",
    "        - [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa *et al.*, JMLR 12, pp. 2825-2830, 2011.\n",
    "        - Jiangang Hao, et al. \"A Review of Scikit-learn Package in Python Programming Language.\" [*Journal of Education and Behavioral Statistics* **Volume: 44 issue: 3** (2019), page(s): 348-361](https://doi.org/10.3102/1076998619832248.)\n",
    "     - [**keras**](https://keras.io/): Book chapter: \"An Introduction to Deep Learning and Keras\" in [*Learn Keras for Deep Neural Networks* (2019), **page(s):1-16**](https://doi.org/10.1007/978-1-4842-4240-7).\n",
    "     - [**Matplotlib**](https://matplotlib.org/)\n",
    "     - [**timeit**](https://docs.python.org/3/library/timeit.html)\n",
    "     - `smiles encoder` function: Blogpost by iwatobipen, *encode and decode SMILES strings* , [Wordpress, accessed November 9th, 2020](https://iwatobipen.wordpress.com/2017/01/22/encode-and-decode-smiles-strings/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecular data and representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChEMBL database\n",
    "\n",
    "- [ChEMBL](https://www.ebi.ac.uk/chembl/) is an open large-scale bioactivity database, containing molecules with drug-like properties. \n",
    "- Recent release 17 contains information extracted from  more than 51,000 publications, together with bioactivity   data sets from 18 other sources (depositors and databases). In total, there are now more than 1.3 million distinct compound structures and 12 million bioactivity data points.\n",
    "- It is maintained by [European Bioinformatics Institute](https://en.wikipedia.org/wiki/European_Bioinformatics_Institute).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMILES structures and rules\n",
    "- [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) (Simplified Molecular Input Line Entry System) notation is a chemical notation that allows a user to represent a chemical structure of a molecule in a linear way that can be read by the computer (see \"Modern Aspects of the Smiles Rearrangement\" (2017), [*Chemistry A European Journal*, **Volume23, Issue38**, 8992-9008](https://doi.org/10.1002/chem.201700353) for further information).\n",
    "- It contains a sequence of letters, numbers and characters that specify a molecule's atoms, their connectivity, bond order and chirality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some SMILES specification rules**\n",
    "\n",
    "- **Atoms** - are represented by their atomic symbols. Also metal atoms are represented with symbols in square bracket, e.g. Gold `[Au]`.\n",
    "- **Bonds** - single, double and triple bonds are represented by symbols `-`, `=` and `#` respectively. Single bonds are the default and therefore do not need to be specified. \n",
    "- **Aromaticity** - While atomic symbols are generally used in upper case, such as C, O, S and N; to specify aromatic atoms lower case symbols are used instead, such as 'c', 'o', 's' and 'n'. Sometimes implicit bonds in rings (alternating `=` and `-`) are also used to describe aromatic atoms such as C1=CC=CC=C1.\n",
    "- **Rings** - SMILES allows a user to identify ring structures by using numbers to identify the opening and closing ring atom. For example, in `C1CCCCC1`, the first carbon has a number '1' which connects by a single bond with the last carbon which also has a number '1'. The resulting structure is cyclohexane.\n",
    "- **Branches** - are specified by enclosing them in parentheses, and can be nested or arranged. For example, 2-Propanol is represented by `CC(O)C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is categorical data?\n",
    "Categorical data are variables that contain labels rather than numeric values.\n",
    "Some examples include:\n",
    "\n",
    "- A “pet” variable with the values: “dog” and “cat“.\n",
    "- A “color” variable with the values: “red“, “green” and “blue“.\n",
    "- A “place” variable with the values: “first”, “second” and “third“."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the problem with categorical data?\n",
    "Machine Learning is, after all, a bunch of mathematical operations translated to a computer via low-level programming languages. Computers are brilliant when dealing with numbers. So we must somehow convert our input data to numbers. \n",
    "There are many machine learning algorithms which cannot operate on categorical data directly so they must be converted to a numerical form so all our input variables and output variables will be numeric (see [**Blogpost**](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/): Alakh Sethi, *One-Hot Encoding vs. Label Encoding using Scikit-Learn* , Analytics Vidya, accessed March 6th, 2020 for further information).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/external-content.duckduckgo.com_.jpeg\" alt=\"Drawing\" style=\"max-width: 500px; width:400%;\"/>\n",
    "</div>\n",
    "\n",
    "*Figure 2*: Displays the categorical encoding required for computers to understand the input. The figure comes from this [blogpost](https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to convert categorical data to numerical data?\n",
    "There are many ways to convert categorical values into numerical values. Each approach has its own positive and negative impact on the feature set. Hereby, two main methods will be the focus: `One-Hot` encoding and `Label` encoding.\n",
    "Both of these encoders are part of the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) library (one of the most widely used Python libraries) and are used to convert text or categorical data into numerical data which the model expects and and can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The *One-Hot Encoding* concept\n",
    "The one-hot encoding is a vector representation where all the elements of the vector are set to `0` except one, which has `1` as its value. For example, `[0 0 0 1 0 0]` is a one-hot vector.\n",
    "Simply put, one-hot encoding, also known as binary encoding, is a binary representation of categorical variables as binary vectors. \n",
    "\n",
    "The figure shown below helps us gain an overall idea of the one-hot encoding concept. \n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/OneHotEncoding.png\" style=\"max-width: 500px; width:100%;\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "*Figure 3* : Displays the one-hot encoding of the toluene molecule. Figure taken from the article [<i>BMC Bioinformatics.</i> (2018), <b>19</b>,526](https://doi.org/10.1186/s12859-018-2523-5), more information can be found there.\n",
    " \n",
    "\n",
    "Let us take a deeper look into the concept with the help of a simple example that will describe the basic concept of one-hot encoding, why it is useful and how one can approach it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why using one-hot encoding?\n",
    "One-hot encoding allows the representation of categorical data to be more expressive.\n",
    "It is difficult for many machine learning algorithms to work with categorical data directly that's why the label values which are categorical must be converted into numbers first as a preprocessing step. This is required for both input and output variables that are categorical.\n",
    "\n",
    "We could also use an *integer encoding* directly. This may work for problems where there is a natural ordinal relationship between the categories, and in turn the integer values, such as labels for temperature ‘cold’, warm’, and ‘hot’.\n",
    "There may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship by using integer encoding might be not useful to solve the problem. An example might be the labels ‘dog’ and ‘cat’.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of one-hot encoding\n",
    "Let us take a look at a very simple example to understand this concept. Assume we have the “color” variable which has three labels `RED` , `BLUE` and `GREEN`.\n",
    "All these labels must be converted into numeric form in order to work with our machine learning algorithm. This can be done by creating three new columns having all three labels and use `1` for the color of the respective label and `0` for the other colors as shown in Figure 4.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/OneHotEncoding_eg.png\" style=\"max-width: 700px; width:200%;\" />\n",
    "</div>\n",
    "\n",
    "*Figure 4* : The visual demonstration of one-hot encoding done on the variable \"color\". Figure taken from the article: \"*Building a One Hot Encoding Layer with TensorFlow*\", George Novack, [towardsdatascience](https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow-f907d686bf39), more details can be found there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Advantages of one-hot encoding \n",
    "-  If the cardinality (the number of categories) of the categorical features is low (relative to the amount of data), one-hot encoding will work best.\n",
    "-  One hot encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Disadvantages of one-hot encoding \n",
    "-  Increase in dimensionality, after adding several columns based on categorical variables which may result in an increase in the computational cost.\n",
    "- There is a high chance of multi-collinearity due to dummy variables which can affect the performance of our model.\n",
    "-  One-hot encoding can result in increasing the [sparsity](https://en.wikipedia.org/wiki/Sparse_matrix) of a dataset (a sparse matrix is a matrix in which most of the elements are zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other similar transformation: Integer encoding (label encoder)\n",
    "\n",
    "[Label Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html), or integer encoding, is a popular encoding technique for handling categorical variables and is easily reversible. In this technique, each label is assigned a unique integer based on alphabetical ordering, so that machines can work with it properly.\n",
    "Machine learning algorithms can then decide in a better way on how labels must be operated on.\n",
    "It is an important preprocessing step for the structured dataset in supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of integer encoding**\n",
    "\n",
    "Let us take a similar example as above: we have a color variable and we can assign `red` as `0`, `green` as `1`, and `blue` as `2` as shown in Figure 5.\n",
    "\n",
    "\n",
    "\n",
    "![OneHotEncoding Example](images/label_encoding_example.png)\n",
    "\n",
    "*Figure 5* : The visual demonstration of label encoding done on the variable \"color\". Figure taken from the article: \"*Know about Categorical Encoding, even New Ones!*\", Ahmed Othmen, [towardsdatascience](https://towardsdatascience.com/know-about-categorical-encoding-even-new-ones-c266227b9cbd), more details can be found there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between label and one-hot encoding\n",
    "\n",
    "There is not much difference between these two encoding techniques, it mainly depends on the type of data and model we are using. For example if we have categorical features which are not ordinal (dog or cat) then we can use one-hot encoding. Label encoding works best with ordinal data like `Good=0, Better=1, Best=2`.\n",
    "Also when there are more categorical variables then it might be good to choose label encoding just to avoid high memory consumption and sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding \n",
    "\n",
    "**What is padding?**\n",
    "\n",
    "Additionally, we will perform padding in the practical section which is used to add zeros on the resulted one-hot encoded matrix. There are different types of padding, we chose to performed zero padding in here. For more details, please refer to this [Blogpost](https://www.machinecurve.com/index.php/2020/02/07/what-is-padding-in-a-neural-network/#:~:text=Padding%20avoids%20the%20loss%20of%20spatial%20dimensions,-Sometimes%2C%20however%2C%20you&text=You%20need%20the%20output%20images,in%20order%20to%20generate%20them.).\n",
    "\n",
    "**Why is it performed?**\n",
    "\n",
    "- Padding is performed to make the dimensions of the matrix equal or to say preserve the height and the width and don't have to worry too much about tensor dimensions when used as an input for the deep learning models.\n",
    "- It avoids the loss of spatial dimensions.\n",
    "- It also allows to build deeper networks because without padding reduction in volume size would reduce too quickly.\n",
    "\n",
    "\n",
    "**How is it performed?**\n",
    "\n",
    "It can be performed by using the [numpy.pad](https://numpy.org/doc/stable/reference/generated/numpy.pad.html) function which takes several parameters like the `array` which needs to be padded, `pad_width` which is number of values added to the edges of each axis and `mode` which by default is 'constant'.\n",
    "\n",
    "In this talktorial, *padding* is performed\n",
    "- *implicitly*: when applying one-hot encoding using our own implementation on the preprocessed data, where we have given the maximum length of the string as the parameter so that all the resulting one-hot encoded matrices are of the same dimension\n",
    "- *explicitly*: when applying one-hot encoding using *keras* and under two scenarios when using *scikit-learn*. Padding is performed\n",
    "    - *before one-hot encoding* : In this scenario, a function is defined to first pad the label encoded characters of the SMILES string and then perform one-hot encoding using scikit-learn.\n",
    "    - *after one-hot encoding*: Whereas in this scenario, a function was defined to one-hot encode all the SMILES string and then perform padding on the resulted one-hot encoded matrix.\n",
    "\n",
    "Both of these paddings are performed to compare the time difference in achieving one-hot encoding when no padding is performed and when the padding is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further readings\n",
    "\n",
    "This section lists some resources for further reading:\n",
    "\n",
    "- [What is one-hot encoding and when is it used in data science?](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science)\n",
    "- [Categorical encoding using Label-Encoding and One-Hot-Encoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd#:~:text=One%2DHot%20Encoding%20in%20Python&text=OneHotEncoder%20from%20SciKit%20library%20only,apply%20OneHotEncoder%20on%20column%20Bridge_Types_Cat.)\n",
    "- Hirohara, M., Saito, Y., Koda, Y. et al. Convolutional neural network based on SMILES representation of compounds for detecting chemical motif. [_BMC Bioinformatics_ **19**, 526 (2018)](https://doi.org/10.1186/s12859-018-2523-5)\n",
    "- [How one can use matplotlib.pyplot.imshow() in Python](https://www.geeksforgeeks.org/matplotlib-pyplot-imshow-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, PandasTools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Silence some expected warnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "DATA = HERE / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, draw molecules and visualize the dataframe\n",
    "\n",
    "Using the `pandas` library, we first load the subset of the ChEMBL dataset and draw the molecules using the `rdkit.draw` function. We then apply different implementations of `one-hot encoding` to the SMILES structures.\n",
    "\n",
    "Let's load the data and quickly analyze its column values and check if there are any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA/'CHEMBL25_activities_EGFR.csv',\n",
    "                 lineterminator='\\n', index_col=0)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimension and missing value of the data\n",
    "print(\"Shape of dataframe : \", df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 3 rows\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns which are necessary\n",
    "df = df[['chembl_id', 'canonical_smiles']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the molecules with their ChEMBL ID using pandas tools and the `draw` method as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas tools and the draw method\n",
    "PandasTools.AddMoleculeColumnToFrame(df, smilesCol='canonical_smiles', molCol='Mol2D' )\n",
    "Draw.MolsToGridImage(list(df.Mol2D[0:10]),\n",
    "                     legends=list(df.chembl_id[0:20]),\n",
    "                     molsPerRow=5)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using own implementation\n",
    "\n",
    "First we created a list of all the possible characters which can be present in the SMILES strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all possible SMILES characters\n",
    "SMILES_CHARS = [' ', '#', '%', '(',\n",
    "                    ')', '+', '-', '.', '/', '0', '1', '2', '3',\n",
    "                    '4', '5', '6', '7', '8', '9',\n",
    "                    '=', '@', 'A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M',\n",
    "                    'N', 'O', 'P', 'R', 'S', 'T', 'V', 'X', 'Z',\n",
    "                    '[', '\\\\', ']', 'a', 'b', 'c', 'e', 'g', 'i',\n",
    "                    'l', 'n', 'o', 'p', 'r', 's', 't', 'u']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a function named `smiles_encoder` is defined which will be useful to create the one-hot encoded matrix of the SMILES strings, it takes SMILES string and SMILES characters as its parameter and returns the one hot encoded matrix of the SMILES string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined to create one-hot encoded matrix\n",
    "def smiles_encoder(smiles, smiles_char):\n",
    "    \"\"\"\n",
    "    One-hot encodes a given SMILES string\n",
    "    using the given pre-defined characters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "          SMILES string of a compound.\n",
    "    smiles_char : list\n",
    "          list of all possible SMILES characters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smiles_matrix: ndarray\n",
    "          One-hot encoded matrix of shape\n",
    "          (`smiles_char`, `len(smiles)`).\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the dataset into a dictionary\n",
    "    smi2index = dict((char, index) for index, char in enumerate(smiles_char))\n",
    "    # one-hot encoding\n",
    "    smiles_matrix = np.zeros((len(smiles_char), len(smiles)), dtype=int)\n",
    "    for index, char in enumerate(smiles):\n",
    "        smiles_matrix[smi2index[char], index] = 1\n",
    "    return smiles_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the canonical SMILES strings in the dataframe\n",
    "start = timer()\n",
    "df['own_ohe_matrix'] = df['canonical_smiles'].apply(smiles_encoder, smiles_char=SMILES_CHARS)\n",
    "end = timer()\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "smiles_encoder_time = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{smiles_encoder_time:.2f} secs\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of one-hot encoded matrix (equal dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Matplotlib` is a plotting library for the python programming language and `Pyplot` is a state-based interface to a matplotlib module which provides a MATLAB-like interface.\n",
    "The [imshow()](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.imshow.html) function in the pyplot module of the matplotlib library is used to display data as an image i.e. on a 2D space.\n",
    "\n",
    "We now visualize our one-hot encoded matrix using `imshow()` by defining the `one_hot_matrix_plot` function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix_plot(ohe_matrix, smiles_char, smiles):\n",
    "    \"\"\"\n",
    "    Visualize one-hot encoded matrix\n",
    "    using matplotlib imshow() function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    one_hot_matrix_plot : ndarray\n",
    "       One-hot encoded (ohe) matrix of shape\n",
    "       (`smiles_char`, `len(smiles)`).\n",
    "    smiles_char : list\n",
    "        list of all possible SMILES characters.\n",
    "    smiles : string\n",
    "        SMILES string of respective molecule        \n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    im = plt.imshow(ohe_matrix, cmap='hot', interpolation='None')\n",
    "    plt.xlabel('Length of SMILES string')\n",
    "    plt.ylabel(f'Char in SMILES ({len(smiles_char)})')\n",
    "    plt.title('Visualization of one-hot encoded matrix')\n",
    "    plt.show()\n",
    "    print('Shape of one-hot matrix : ', ohe_matrix.shape)\n",
    "    print('Associated canonical SMILES: ', smiles)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can choose the index of the compound of your own choice for which you want to visualize the one-hot encoded matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index \n",
    "index = 0\n",
    "one_hot_matrix_plot(df.iloc[index]['own_ohe_matrix'], SMILES_CHARS, \n",
    "                    df.iloc[index]['canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, we wanted to find and draw the shortest and the longest SMILES string and visualize their matrix. It was achieved by using `max` and `min` functions on the *canonical_smiles* column as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the longest smile string\n",
    "longest_smile = max(df[\"canonical_smiles\"], key = len) \n",
    "longest_smile_index = df.canonical_smiles[df.canonical_smiles == longest_smile].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the shortest smile string\n",
    "shortest_smile = min(df[\"canonical_smiles\"], key = len) \n",
    "shortest_smile_index = df.canonical_smiles[df.canonical_smiles == shortest_smile].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now know the indices of the longest and shortest SMILES strings, we can pass these indices to the `one_hot_matrix_plot` function and visualize the matrices. We can also draw the molecules using the `Chem.MolFromSmiles` function from rdkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the index of the longest smile string to visualize the matrix\n",
    "one_hot_matrix_plot(df.iloc[longest_smile_index[0]]['own_ohe_matrix'], SMILES_CHARS, \n",
    "                    longest_smile)  # NBVAL_CHECK_OUTPUT\n",
    "\n",
    "# Draw the longest smile molecule\n",
    "longest_smile_mol = Chem.MolFromSmiles(longest_smile)\n",
    "longest_smile_mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the index of the shortest smile string to visualize the matrix\n",
    "one_hot_matrix_plot(df.iloc[shortest_smile_index[0]]['own_ohe_matrix'], SMILES_CHARS, \n",
    "                    shortest_smile)  # NBVAL_CHECK_OUTPUT\n",
    "\n",
    "# Draw the shortest smile molecule\n",
    "shortest_smile_mol = Chem.MolFromSmiles(shortest_smile)\n",
    "shortest_smile_mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, the matrix visualization was performed using matplotlib `imshow()` function, we can also visualize the entire matrix using the [numpy.matrix](https://numpy.org/doc/stable/reference/generated/numpy.matrix.html) function, e.g. the one-hot encoded matrix of the longest SMILES string as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the some rows of the matrix for the longest smile\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "subset=5\n",
    "print(f'First {subset} rows of the ohe matrix, representing the charcters {SMILES_CHARS[0:subset]}\\n')\n",
    "print(np.matrix(df.iloc[longest_smile_index[0]]['own_ohe_matrix'])[0:subset, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of shortcomings of defined `smiles_encoder` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**and solutions to overcome those**\n",
    "- First, as we notice from the visualizations above, SMILES strings could have unequal dimension since their string length might differ. For machine learning application, having equal dimension throughout the data set is required. In order to achieve this, we can first search for the SMILES string with the maximum length (e.g. [len()](https://www.geeksforgeeks.org/python-string-length-len/) method) and pass it as an argument in our function for all the strings.\n",
    "\n",
    "- Second, in the above function, we have created a dataset of 56 possible SMILES characters, but to optimize one-hot encoding, we considered reducing this character set. Searching for all the unique characters present in the SMILES data set will allow us to remove the characters not present in the current data.\n",
    "\n",
    "- Third, many elements in the periodic table are represented by two characters, e.g. 'Cl' stands for chloride. This double-character phenomenon will be misinterpreted by the above `smiles_encoder` function, i.e. it would split 'Cl' into two characters, namely 'C' and 'l', and that could lead to discrepancies, so searching for each unique character and encoding them may not be optimal. Hence, we suggest here to search for all double-character elements in our SMILES data set by comparing the atoms present in our strings with all the possible elements present in the periodic table and replacing all the two alphabetic elements with one character, for example changing 'Cl' to 'L'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process SMILES data to overcome shortcomings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the `preprocessing_data` function to pre-process the SMILES strings by implementing the above discussed shortcomings.\n",
    "\n",
    "**Note:** This function is highly specific to the dataset used in this talktorial, because we replace two letter elements that were found in this particular dataset. One can adapt the function and add more two letter elements if found when using another dataset and convert them into single letter element accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_two_letter_elements(df):\n",
    "    \"\"\"\n",
    "    Find the two letter elements in dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "       Dataframe which requires preprocessing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    two_letter_elements : list\n",
    "        List with found two letter elements\n",
    "    \"\"\"\n",
    "    # Search for unique characters in our SMILES strings\n",
    "    unique_chars = set(df.canonical_smiles.apply(list).sum())\n",
    "    upper_chars = [] \n",
    "    lower_chars = [] \n",
    "    for entry in unique_chars:\n",
    "        if entry.isalpha():\n",
    "            if entry.isupper():\n",
    "                upper_chars.append(entry)\n",
    "            elif entry.islower():\n",
    "                lower_chars.append(entry)\n",
    "    print(f'Upper letter characters {upper_chars}')\n",
    "    print(f'Lower letter characters {lower_chars}')\n",
    "    \n",
    "    # List of all possible periodic elements\n",
    "    periodic_elements = ['Ac',\n",
    "                         'Al', 'Am', 'Sb', 'Ar', 'As', 'At', 'Ba',\n",
    "                         'Bk', 'Be', 'Bi', 'Bh', 'B', 'Br', 'Cd', 'Ca',\n",
    "                         'Cf', 'C', 'Ce', 'Cs', 'Cl', 'Cr', 'Co', 'Cn',\n",
    "                         'Cu', 'Cm', 'Ds', 'Db', 'Dy', 'Es', 'Er', 'Eu',\n",
    "                         'Fm', 'Fl', 'F', 'Fr', 'Gd', 'Ga', 'Ge', 'Au',\n",
    "                         'Hf', 'Hs', 'He', 'Ho', 'H', 'In', 'I', 'Ir',\n",
    "                         'Fe', 'Kr', 'La', 'Lr', 'Pb', 'Li', 'Lv',\n",
    "                         'Lu', 'Mg', 'Mn', 'Mt', 'Md', 'Hg', 'Mo',\n",
    "                         'Mc', 'Nd', 'Ne', 'Np', 'Ni', 'Nh', 'Nb',\n",
    "                         'N', 'No', 'Og', 'Os', 'O', 'Pd', 'P',\n",
    "                         'Pt', 'Pu', 'Po', 'K', 'Pr', 'Pm', 'Pa',\n",
    "                         'Ra', 'Rn', 'Re', 'Rh', 'Rg', 'Rb', 'Ru', 'Rf',\n",
    "                         'Sm', 'Sc', 'Sg', 'Se', 'Si', 'Ag', 'Na',\n",
    "                         'Sr', 'S', 'Ta', 'Tc', 'Te', 'Ts', 'Tb', 'Tl', 'Th',\n",
    "                         'Tm', 'Sn',\n",
    "                         'Ti', 'W', 'U', 'V', 'Xe', 'Yb', 'Y', 'Zn', 'Zr']\n",
    "\n",
    "    # 'two_char_elements' is a list that contains two letter elements\n",
    "    # which are valid when compared with all the possible periodic elements.\n",
    "    two_char_elements = []\n",
    "    for upper in upper_chars:\n",
    "        for lower in lower_chars:\n",
    "            ch = upper + lower\n",
    "            if (ch in periodic_elements):\n",
    "                two_char_elements.append(ch)\n",
    "\n",
    "    # 'two_charc_elements_smiles' is a set that contains all the possible\n",
    "    # two letter elements in our SMILES strings, that is specific to our dataset.\n",
    "    two_char_elements_smiles = set()\n",
    "    for char in two_char_elements:\n",
    "        for index in range(len(df)):\n",
    "            if (df['canonical_smiles'].iloc[index].find(char) != -1):\n",
    "                two_char_elements_smiles.add(char)\n",
    "    return(two_char_elements_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_found = assess_two_letter_elements(df)\n",
    "print(f'Two letter elements found in data set: {elements_found}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this finding we defined our own dictionary for replacement. Note that\n",
    "- We excluded *Sc* from replacement, since it is more likely that sulfur (S) and and aromatic carbon (c) are contained in a molecule than scandium (Sc).\n",
    "- In isomeric SMILES `@` and `@@` are used to describe enantiomers, thus we also need to replace the latter by a one letter code.\n",
    "- If you are working wit a different data set, you may want to adapt the below mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to replace the two letter elements found\n",
    "replace_dict ={'Cl':'L', 'Br':'R' ,'Cn':'X' , 'Se':'Z' , '@@':'$'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to create the preprocessed data\n",
    "def preprocessing_data(df, replacement):\n",
    "    \"\"\"\n",
    "    Preprocess the SMILES structures in a data set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "       Dataframe which requires preprocessing.\n",
    "    replacement : dict\n",
    "       Dictionary with mapping for replacement.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with new processed canonical SMILES column.\n",
    "    unique_char : list\n",
    "        List with unique characters present in SMILES.\n",
    "    SMILES_maxlen : int\n",
    "        Maximum length of SMILES string.\n",
    "    \"\"\"\n",
    "    # Print warning if the dataset has a 'Sc' element\n",
    "    for tmp_smiles in df.canonical_smiles:\n",
    "        if (tmp_smiles.find('Sc') != -1): \n",
    "            print('**Warning**: \"Sc\" element is found in the dataset, since the element is rarely found '\n",
    "                  'in the drugs so we are not converting  '\n",
    "                  'it to single letter element, instead considering \"S\" '\n",
    "                  'and \"c\" as seperate elements. ')\n",
    "            break\n",
    "\n",
    "    \n",
    "    # Create a new column having processed canonical SMILES\n",
    "    df['processed_canonical_smiles'] = \"\"\n",
    "\n",
    "    # Replace the two letter elements found\n",
    "    # with one character\n",
    "    for index in range(len(df)):\n",
    "        cur_smiles = df['canonical_smiles'].iloc[index]\n",
    "        for key, value in replacement.items():\n",
    "            cur_smiles = cur_smiles.replace(key,value)\n",
    "        df['processed_canonical_smiles'].iloc[index] = cur_smiles\n",
    "    \n",
    "    # Calculate max length of the SMILES strings\n",
    "    SMILES_maxlen = df[\"processed_canonical_smiles\"].str.len().max()\n",
    "    \n",
    "    unique_char = set(df.processed_canonical_smiles.apply(list).sum())\n",
    "    return unique_char, df, SMILES_maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using own implementation on preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function\n",
    "unique_char, df, SMILES_maxlen = preprocessing_data(df, replace_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new version of `smiles_encoder` function is defined which is named as `new_smiles_encoder` with an additional parameter of *maxlen* which takes the maximum length of the SMILES string and *uniquechar* which takes the list of unique characters present in the *processed canonical string*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined to create one-hot encoded matrix\n",
    "def new_smiles_encoder(smiles, maxlen, uniquechar):\n",
    "    \"\"\"\n",
    "    Function defined using all unique characters in our\n",
    "    processed canonical smiles structures created\n",
    "    from preprocessed function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "         SMILES data in string format.\n",
    "    uniquechar : list\n",
    "         list of unique characters in the string dataset.\n",
    "    maxlen : int\n",
    "         maximum length of the SMILES string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smiles_matrix : ndarray\n",
    "         One-hot encoded matrix of fixed shape\n",
    "         (unique char in smiles, max smile length).\n",
    "    \"\"\"\n",
    "    # create dictionary of the unique char dataset\n",
    "    smi2index = dict((char, index) for index, char in enumerate(unique_char))\n",
    "    # one hot encoding\n",
    "    smiles_matrix = np.zeros((len(unique_char), maxlen))\n",
    "    for index, char in enumerate(smiles):\n",
    "        smiles_matrix[smi2index[char], index] = 1\n",
    "    return smiles_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the processed canonical SMILES strings\n",
    "start = timer()\n",
    "df['unique_char_ohe_matrix'] = df['processed_canonical_smiles'].apply(\n",
    "                                  new_smiles_encoder, maxlen=SMILES_maxlen, \n",
    "                                  uniquechar=unique_char)\n",
    "end = timer()\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "smiles_encoder_equal_dim = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{smiles_encoder_equal_dim:.2f} secs\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of one-hot encoded matrix (equal dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index of the compound for which the OHE matrix is visualised \n",
    "index = 0\n",
    "one_hot_matrix_plot(df.iloc[index]['unique_char_ohe_matrix'],\n",
    "                    unique_char,\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2705\n",
    "one_hot_matrix_plot(df.iloc[index]['unique_char_ohe_matrix'],\n",
    "                    unique_char,\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above visualizations, we can conclude that the dimensions are equal for all the molecule's one-hot encoded SMILES using `preprocessing_data` and `new_smiles_encoder` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using implementation in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing one hot encoding using scikit-learn, we have defined the functions named `later_padding` which adds horizontal and vertical padding to the given matrix and `initial_padding` which adds zeros to the character list after they are label encoded by using the `numpy.pad` function as discussed in the theory section. \n",
    "\n",
    "It is defined here because it will be used as a boolean parameter (islaterpadding and isinitialpadding) later in the scikit-learn and keras implementations to choose if later padding or initial padding is required or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add padding after one-hot encoding\n",
    "def later_padding(ohe_matrix, maxlen, uniquechar):\n",
    "    \"\"\"\n",
    "    Add horizontal and vertical padding\n",
    "    to the given matrix using numpy.pad() function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ohe_matrix: ndarray\n",
    "        Character array.\n",
    "    smiles_max_len: int\n",
    "        Maximum length of the SMILES string.\n",
    "    unique_char: list\n",
    "        List of unique characters in the string dataset.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    padded_matrix: ndarray\n",
    "           Padded one-hot encoded matrix of\n",
    "           shape (unique char in smiles, max smile_length).\n",
    "   \"\"\"\n",
    "    \n",
    "    padded_matrix = np.pad(ohe_matrix, ((0, SMILES_maxlen-len(ohe_matrix)),\n",
    "                                         (0, len(unique_char)-len(\n",
    "                                          ohe_matrix[0]))), 'constant')\n",
    "    return padded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add padding before one-hot encoding\n",
    "# after label (integer) encoding\n",
    "def initial_padding(smiles, maxlen):\n",
    "    \"\"\"\n",
    "    Add zeroes to the list of characters \n",
    "    after integer encoding them\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "       SMILES string.\n",
    "    maxlen: int\n",
    "       Maximum length of the SMILES string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "      One-hot encoded matrix with intial padding of shape\n",
    "      (unique char in smiles, max smile_length).\n",
    "    \"\"\"\n",
    "    canonical_char = list(smiles)\n",
    "    # perform padding on the list of characters\n",
    "    canonical_char_padded = np.pad(canonical_char, (0,\n",
    "                         maxlen-len(canonical_char)), 'constant')\n",
    "    return canonical_char_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we proceed with our second implementation of one-hot encoding from scikit-learn. We can use [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) from the `sklearn` library but it only takes numerical categorical values, hence any value of string type should be [label_encoded](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) first before one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the functions below first give integer encoded SMILES of the labels and finally one-hot encode the SMILES structures.\n",
    "\n",
    "By default, the OneHotEncoder class will return a more efficient sparse encoding which can be useful in some applications but in this case, we disabled the sparse return type by setting the `sparse=False` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Scikit-learn implementation of one-hot encoding\n",
    "def sklearn_one_hot_encoded_matrix(smiles, islaterpadding, isinitialpadding):\n",
    "    \"\"\"\n",
    "    Label and one-hot encodes the SMILES\n",
    "    using sklearn LabelEncoder and OneHotEncoder implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "        SMILES string of a compound.\n",
    "    islaterpadding : bool\n",
    "        Paramater is `True` if `later_padding` is required,\n",
    "        `False` otherwise.\n",
    "    isinitialpadding : bool\n",
    "        Paramater is `True` if `initial_padding` is required,\n",
    "        `False` otherwise.\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    onehot_encoded : ndarray\n",
    "        One-hot encoded matrix of shape\n",
    "        (chars in individual SMILES, length of individual SMILES).\n",
    "    \"\"\"\n",
    "    # Integer encoding\n",
    "    canonical_char = list(smiles)\n",
    "    label_encoder = LabelEncoder()\n",
    "    # Fit_transform function is used to first fit the data and then transform it\n",
    "    integer_encoded = label_encoder.fit_transform(canonical_char)\n",
    "    \n",
    "    if (isinitialpadding == True):\n",
    "        integer_encoded = initial_padding(integer_encoded, SMILES_maxlen)\n",
    "        \n",
    "    # One-hot encoding\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "    if (islaterpadding == True):\n",
    "        onehot_encoded = later_padding(onehot_encoded, SMILES_maxlen, unique_char)\n",
    "    \n",
    "    onehot_encoded = onehot_encoded.transpose()\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without padding (unequal dimension)\n",
    "\n",
    "We can use the `sklearn_one_hot_encoded_matrix` function defined above to create the one-hot encoded matrix and no padding is perfomed, it will create unequal dimensions of the matrix because it will first label encode all the characters present in the SMILES strings (individually) and then one-hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the processed canonical SMILES strings\n",
    "start = timer()\n",
    "df['sklearn_ohe_matrix_no_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                                          sklearn_one_hot_encoded_matrix, \n",
    "                                          islaterpadding=False, \n",
    "                                          isinitialpadding=False)\n",
    "end = timer()\n",
    "df.head(2) # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "sklearn_time_without_padding = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{sklearn_time_without_padding:.2f} secs\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization with Matplotlib.imshow() function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index of the compound for which the OHE matrix is visualised \n",
    "index = 0\n",
    "one_hot_matrix_plot(df.iloc[index]['sklearn_ohe_matrix_no_padding'],\n",
    "                    df.iloc[index]['sklearn_ohe_matrix_no_padding'],\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "one_hot_matrix_plot(df.iloc[index]['sklearn_ohe_matrix_no_padding'],\n",
    "                    df.iloc[index]['sklearn_ohe_matrix_no_padding'],\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be confirmed from the above visualizations that the one-hot encoded matrix from the sklearn implementation does not have equal dimensions because sklearn's `OneHotEncoder` function takes individual strings as an input.\n",
    "\n",
    "So we equalize dimensions of the one-hot encoded matrix by adding **padding** as described above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With padding (equal dimension)\n",
    "It can be either done after one-hot encoding is performed on the SMILES strings or before, meaning after we label encode the SMILES characters. We discuss both scenarios in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding after one-hot encoding is performed\n",
    "\n",
    "We simply pass **True** to the `islaterpadding` boolean parameter in `sklearn_one_hot_encoded_matrix` function as shown below to pad the matrix after one hot encoding is performed,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the processed canonical smiles strings\n",
    "start = timer()\n",
    "df['sklearn_ohe_matrix_later_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                                             sklearn_one_hot_encoded_matrix, \n",
    "                                             islaterpadding=True, isinitialpadding=False)\n",
    "end = timer()\n",
    "df.head(2) # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "sklearn_time_later = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{sklearn_time_later:.2f} secs\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization with Matplotlib.imshow() function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index of the compound for which the OHE matrix is visualised \n",
    "index = 0\n",
    "one_hot_matrix_plot(df.iloc[index]['sklearn_ohe_matrix_later_padding'],\n",
    "                    unique_char,\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2705\n",
    "one_hot_matrix_plot(df.iloc[index]['sklearn_ohe_matrix_later_padding'],\n",
    "                    unique_char,\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe that the dimensions (37, 267) are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding before one-hot encoding is performed\n",
    "\n",
    "In this case, padding is performed after label or integer encoding the list of SMILES characters by passing **True** to the `initial_padding` boolean parameter in `sklearn_one_hot_encoded_matrix` function, afterwards we defined a function named `initial_padded_encoding` which one hot encodes padded integer encoded SMILES strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_padded_encoding(smiles, uniquechar, maxlen):\n",
    "    \"\"\"\n",
    "    One-hot encodes initially padded SMILES strings\n",
    "    using sklearn_OneHotEncode function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "       SMILES string.\n",
    "    uniquechar: list\n",
    "       List of unique characters in the string dataset.\n",
    "    maxlen: int\n",
    "       Maximum length of the SMILES string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "      One-hot encoded matrix with intial padding of shape\n",
    "      (unique char in smiles, max smile_length).\n",
    "    \"\"\"\n",
    "    # Calling 'sklearn_one_hot_encoded_matrix' function by passing \n",
    "    # False to 'later_padding' and True to 'initial_padding'\n",
    "    sklearnOHC = sklearn_one_hot_encoded_matrix(smiles, False, True)\n",
    "    sklearnOHC = sklearnOHC.transpose()\n",
    "    column_padding = np.ndarray(shape=(SMILES_maxlen,\n",
    "                                len(unique_char)-len(sklearnOHC[0])))\n",
    "    column_padding.fill(0)\n",
    "    \n",
    "    sklearnOHC = np.append(sklearnOHC, column_padding, axis=1)\n",
    "    return sklearnOHC.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the processed canonical SMILES strings\n",
    "start = timer()\n",
    "df['sklearn_ohe_matrix_initial_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                                               initial_padded_encoding, \n",
    "                                               uniquechar = unique_char, \n",
    "                                               maxlen = SMILES_maxlen)\n",
    "\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "sklearn_time_initial = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{sklearn_time_initial:.2f} secs\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization with Matplotlib.imshow() function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose index of the compound for which the OHE matrix is visualised \n",
    "index = 0\n",
    "one_hot_matrix_plot(df.iloc[index]['sklearn_ohe_matrix_initial_padding'],\n",
    "                    unique_char,\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2705\n",
    "one_hot_matrix_plot(df.iloc[index]['sklearn_ohe_matrix_initial_padding'],\n",
    "                    unique_char,\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both initial and later padding was performed to check if there is any major time difference in executing one hot encoding using scikit-learn implementation and if there is any change in the resulted one hot encoded matrices.\n",
    "\n",
    "Results will be discussed in the discussion section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding using implementation in keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is also a very powerful and intensively used library, especially employed in deep learning tasks. \n",
    "There may be a case where we have sequences or strings that are already integer encoded, then we can use the function called [to_categorical()](https://keras.io/api/utils/), provided by the keras library, to one-hot encode integer data directly, otherwise we can use [Tokenizer](https://keras.io/api/preprocessing/text/) to first integer encode the string data and then use `to_categorical` function to one-hot encode the string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use keras implementation of one-hot encoding\n",
    "def keras_one_hot_encoded_matrix(smiles, islaterpadding, maxlen, uniquechar):\n",
    "    \"\"\"\n",
    "    One-hot encodes the SMILES using keras\n",
    "    implementation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    canonical_char : array\n",
    "        Canonical character array.\n",
    "    islaterpadding : bool\n",
    "        The paramater is `True` if later_padding is required,\n",
    "        `False` otherwise.\n",
    "    maxlen: int\n",
    "        Maximum length of the SMILES string.\n",
    "    uniquechar: list\n",
    "        List of unique characters in the string dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    encoded : ndarray\n",
    "        One-hot encoded matrix of shape\n",
    "        (chars in SMILES, length of SMILES).\n",
    "    \"\"\"\n",
    "   \n",
    "    # Integer encoding using Tokenizer\n",
    "    input = smiles\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts([input])\n",
    "    integer_encoded = tokenizer.texts_to_sequences([input])[0]\n",
    "    \n",
    "    # One-hot encoding using to_categorical function\n",
    "    encoded = to_categorical(integer_encoded)\n",
    "    if (islaterpadding == True):\n",
    "        encoded = later_padding(encoded, SMILES_maxlen,unique_char)\n",
    "    encoded = encoded.transpose()\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement two scenarios, \n",
    "- one, when there is no later padding performed which will result in unequal dimensions of the produced one-hot encoded matrix and \n",
    "- the other, where later padding will be performed by passing `True` to the boolean parameter `islaterpadding` in the `keras_one_hot_encoded_matrix` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without padding (unequal dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the processed canonical SMILES strings\n",
    "start = timer()\n",
    "df['keras_ohe_matrix_without_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                                            keras_one_hot_encoded_matrix, \n",
    "                                            maxlen = SMILES_maxlen, \n",
    "                                            uniquechar = unique_char, \n",
    "                                            islaterpadding=False)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "keras_time_no_padding = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{keras_time_no_padding:.2f} secs\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization with Matplotlib.imshow() function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index of the compound for which the OHE matrix is visualised \n",
    "index = 0\n",
    "one_hot_matrix_plot(df.iloc[index]['keras_ohe_matrix_without_padding'],\n",
    "                    df.iloc[index]['keras_ohe_matrix_without_padding'],\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "one_hot_matrix_plot(df.iloc[index]['keras_ohe_matrix_without_padding'],\n",
    "                    df.iloc[index]['keras_ohe_matrix_without_padding'],\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With padding (equal dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function over the Processed_canonical_smiles strings\n",
    "start = timer()\n",
    "df['keras_ohe_matrix_padding'] = df['processed_canonical_smiles'].apply(\n",
    "                                    keras_one_hot_encoded_matrix, \n",
    "                                    maxlen = SMILES_maxlen,\n",
    "                                    uniquechar = unique_char, \n",
    "                                    islaterpadding=True)\n",
    "end = timer()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print time required for execution\n",
    "keras_time_padding = end - start\n",
    "print(f\"Time to execute the function: \"\n",
    "      f\"{keras_time_padding:.2f} secs\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization with Matplotlib.imshow() function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose index of the compound for which the OHE matrix is visualised \n",
    "index = 0\n",
    "one_hot_matrix_plot(df.iloc[index]['keras_ohe_matrix_padding'],\n",
    "                    unique_char,\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2705\n",
    "one_hot_matrix_plot(df.iloc[index]['keras_ohe_matrix_padding'],\n",
    "                    unique_char,\n",
    "                    df.iloc[index]['processed_canonical_smiles'])  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframe with all the one-hot encoded matrices\n",
    "df.head(3)  # NBVAL_CHECK_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare times no padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" smiles_encoder_time no padding: \"\n",
    "      f\"{smiles_encoder_time:.2f} secs\")\n",
    "print(f\" sklearn_time_without_padding: \"\n",
    "      f\"{sklearn_time_without_padding:.2f} secs\")\n",
    "print(f\" keras_time_no_padding: \"\n",
    "      f\"{keras_time_no_padding:.2f} secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare times with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" smiles_encoder_equal_dim: \"\n",
    "      f\"{smiles_encoder_equal_dim:.2f} secs\")  \n",
    "print(f\" sklearn_time_later: \"\n",
    "      f\"{sklearn_time_later:.2f} secs\")  \n",
    "print(f\" sklearn_time_initial: \"\n",
    "      f\"{sklearn_time_initial:.2f} secs\")  \n",
    "print(f\" keras_time_padding: \"\n",
    "      f\"{keras_time_padding:.2f} secs\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion \n",
    "\n",
    "- As we notice from the simulations above, the execution time varies with different implementations:\n",
    "   - **Unequal dimension** (when no padding was performed)\n",
    "      - Unexpectedly, our own `new_smiles_encoder` function calculates the results fastest, followed by the implementation in keras and one-hot encoding with the scikit-learn implementation. But since the dimensions differ, machine learning models cannot be applied.\n",
    "    \n",
    "   - **Equal dimension** (when padding was performed)\n",
    "     - Surprisingly, even after creating equal dimensions (adding padding), our own functions `new_smiles_encoder` along with `preprocessing_data` again outperform the other two implementations with respect to execution time, followed by keras and again scikit-learn required most time for execution. Generally, all three executions times increased for the equal dimension experiment. One explanation for this difference may be the additional padding performed on the strings.\n",
    "     \n",
    "       \n",
    "- Note that there is also a small time execution difference using scikit-learn when padding is performed: One possible reason for this time difference could be that if we perform padding after label encoding, we will have more  characters to one-hot encode as compared to just pad after one-hot encoding is accomplished.\n",
    "   \n",
    "- The produced one-hot encoded matrices after performing initial and later padding found to be similar because the used `LabelEncoder` function resulted in same integer encoding of the same SMILES string data which resulted in the same vectors produced for all the characters in the SMILES string in both the scenarios. Hence, the one-hot encoded matrices are similar.\n",
    "\n",
    "**Note:** Execution times might slightly differ depending on the environment and hardware used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were several challenges faced during the task, for instance:\n",
    "\n",
    "- Making equal dimensions of the one-hot encoded matrix.\n",
    "\n",
    "- Replacing two letter elements with unique characters (note: the selected replacement might need to be extended for other datasets).\n",
    "\n",
    "- After searching for two letter elements, we found a specific element `Sc` which is a metallic element. In our dataset both 'S' and 'c' elements are present individually as well. So if we would have replaced `Sc` with single a letter element then it might have effected our actual strings. So we assumed that since `Sc` is a metallic element and is rarely present in SMILES, we did not replace this element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Why is it required to have equal dimensions of the one-hot encoded matrix ?\n",
    "- Is there any other way to pre-process the data ?\n",
    "- How and which machine learning models can be applied on the above dataset ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
